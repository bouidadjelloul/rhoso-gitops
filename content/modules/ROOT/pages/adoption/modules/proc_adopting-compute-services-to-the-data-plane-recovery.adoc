[id="adopting-compute-services-to-the-data-plane_{context}"]

= Adopting Compute services to the {rhos_acro} data plane with a failed Compute Node

Adopt your Compute (nova) services to the {rhos_long} data plane.

//kgilliga: The following text belongs under the code block in step 6 but I'm unable to hide it there: "For multi-cell, config maps and {rhos_prev_long} data plane services should be named like `nova-custom-ceph-cellX` and `nova-compute-extraconfig-cellX`."

.Prerequisites

* You have stopped the remaining control plane nodes, repositories, and packages on the {compute_service_first_ref} hosts. For more information, see xref:stopping-infrastructure-management-and-compute-services_{context}[Stopping infrastructure management and Compute services].
* In the bastion, create the dataplane network (IPAM):
+
[source,bash,role=execute,subs=attributes]
----
cd /home/lab-user/labrepo/content/files/
oc apply -f osp-ng-dataplane-netconfig-adoption.yaml
----
+

* Get the libvirt secret password:
+
[source,bash,role=execute]
----
LIBVIRT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' LibvirtTLSPassword:' | awk -F ': ' '{ print $2; }')
LIBVIRT_PASSWORD_BASE64=$(echo -n "$LIBVIRT_PASSWORD" | base64)
----
+

* Create the libvirt secret:
+
[source,yaml,role=execute]
----
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: libvirt-secret
  namespace: openstack
type: Opaque
data:
  LibvirtPassword: ${LIBVIRT_PASSWORD_BASE64}
EOF
----
+

* You have defined the shell variables to run the script that runs the fast-forward upgrade:
+
[source,bash,role=execute,subs=attributes]
----
PODIFIED_DB_ROOT_PASSWORD=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)

alias openstack="oc exec -t openstackclient -- openstack"
declare -A computes
computes=(
  ["compute02.localdomain"]="172.22.0.110"
  ["compute03.localdomain"]="172.22.0.112"
)
----

.Procedure

ifeval::["{build}" != "downstream"]
. Create a https://kubernetes.io/docs/concepts/configuration/secret/#ssh-authentication-secrets[ssh authentication secret] for the data plane nodes:
//kgilliga:I need to check if we will document this in Red Hat docs.
endif::[]
ifeval::["{build}" != "upstream"]
. Create an SSH authentication secret for the data plane nodes:
endif::[]
+
[source,bash,role=execute,subs=attributes]
----
oc create secret generic dataplane-ansible-ssh-private-key-secret \
--save-config \
--dry-run=client \
--from-file=authorized_keys=/home/lab-user/.ssh/{guid}key.pub \
--from-file=ssh-privatekey=/home/lab-user/.ssh/{guid}key.pem \
--from-file=ssh-publickey=/home/lab-user/.ssh/{guid}key.pub \
-n openstack \
-o yaml | oc apply -f-
----
+
ifeval::["{build}" == "downstream"]
* Replace `/home/lab-user/.ssh/{guid}key.pem` with the path to your SSH key.
endif::[]

. Generate an ssh key-pair `nova-migration-ssh-key` secret:
+
[source,bash,role=execute,subs=attributes]
----
cd "$(mktemp -d)"
ssh-keygen -f ./id -t ecdsa-sha2-nistp521 -N ''
oc get secret nova-migration-ssh-key || oc create secret generic nova-migration-ssh-key \
  -n openstack \
  --from-file=ssh-privatekey=id \
  --from-file=ssh-publickey=id.pub \
  --type kubernetes.io/ssh-auth
rm -f id*
cd -
----

. As we use a local storage back end for libvirt, create a `nova-compute-extra-config` service to remove pre-fast-forward workarounds and configure Compute services to use a local storage back end:
+
[source,bash,role=execute,subs=attributes]
----
cat << EOF | oc apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-extra-config
  namespace: openstack
data:
  19-nova-compute-cell1-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=true
EOF
----
+
[NOTE]
The secret `nova-cell<X>-compute-config` auto-generates for each
`cell<X>`. You must specify values for the `nova-cell<X>-compute-config` and `nova-migration-ssh-key` parameters for each custom `OpenStackDataPlaneService` CR that is related to the {compute_service}.

+
The resources in the `ConfigMap` contain cell-specific configurations.

. Create a secret for the subscription manager:
+
[source,yaml,role=execute]
----
oc create secret generic subscription-manager \
--from-literal rhc_auth='{"login": {"username": "<subscription_manager_username>", "password": "<subscription_manager_password>"}}'
----
+
* Replace `<subscription_manager_username>` with the applicable user name.
* Replace `<subscription_manager_password>` with the applicable password.

. Create a secret for the Red Hat registry:
+
[source,yaml,role=execute]
----
oc create secret generic redhat-registry \
--from-literal edpm_container_registry_logins='{"registry.redhat.io": {"<registry_username>": "<registry_password>"}}'
----
+
* Replace `<registry_username>` with the applicable user name.
* Replace `<registry_password>` with the applicable password.

. Create the `OpenStackDataPlaneNodeSet` CRs corresponding to compute02 and compute03:
+
[source,bash,role=execute,subs=attributes]
----
oc apply -f osp-ng-dataplane-node-set-deploy-adoption-compute-2.yaml
oc apply -f osp-ng-dataplane-node-set-deploy-adoption-compute-3.yaml
----

* Take some time to go through the *osp-ng-dataplane-adoption-compute-2.yaml* and *osp-ng-dataplane-adoption-compute-3.yaml* files. We have configured the edpm_ovn_bridge_mappings with "datacentre:br-ex"

. Run the pre-adoption validation:

.. Create the validation service:
+
[source,bash,role=execute,subs=attributes]
----
cat << EOF | oc apply -f -
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: pre-adoption-validation
spec:
  playbook: osp.edpm.pre_adoption_validation
EOF
----

.. Create a `OpenStackDataPlaneDeployment` CR that runs only the validation:
+
[source,bash,role=execute,subs=attributes]
----
cat << EOF | oc apply -f -
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-pre-adoption
spec:
  nodeSets:
  - compute-2
  - compute-3
  servicesOverride:
  - pre-adoption-validation
EOF
----

.. When the validation is finished, confirm that the status of the Ansible EE pods is `Completed`:
+
[source,bash,role=execute,subs=attributes]
----
watch oc get pod -l app=openstackansibleee
----
+
[source,bash,role=execute,subs=attributes]
----
oc logs -l app=openstackansibleee -f --max-log-requests 20
----

.. Wait for the deployment to reach the `Ready` status:
+
[source,bash,role=execute,subs=attributes]
----
oc wait --for condition=Ready openstackdataplanedeployment/openstack-pre-adoption --timeout=10m
----
+
[IMPORTANT]
====
If any openstack-pre-adoption validations fail, you must reference the Ansible logs to determine which ones were unsuccessful, and then try the following troubleshooting options:

* If the hostname validation failed, check that the hostname of the data plane
node is correctly listed in the `OpenStackDataPlaneNodeSet` CR.

* If the kernel argument check failed, ensure that the kernel argument configuration in the `edpm_kernel_args` and `edpm_kernel_hugepages` variables in the `OpenStackDataPlaneNodeSet` CR is the same as the kernel argument configuration that you used in the {rhos_prev_long} ({OpenStackShort}) {rhos_prev_ver} node.

* If the tuned profile check failed, ensure that the
`edpm_tuned_profile` variable in the `OpenStackDataPlaneNodeSet` CR is configured
to use the same profile as the one set on the {OpenStackShort} {rhos_prev_ver} node.
====

. Remove the remaining {OpenStackPreviousInstaller} services:

.. Create an `OpenStackDataPlaneService` CR to clean up the data plane services you are adopting:
+
[source,bash,role=execute,subs=attributes]
----
cat << EOF | oc apply -f -
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: tripleo-cleanup
spec:
  playbook: osp.edpm.tripleo_cleanup
EOF
----

.. Create the `OpenStackDataPlaneDeployment` CR to run the clean-up:
+
[source,bash,role=execute,subs=attributes]
----
cat << EOF | oc apply -f -
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: tripleo-cleanup
spec:
  nodeSets:
  - compute-2
  - compute-3
  servicesOverride:
  - tripleo-cleanup
EOF
----

. When the clean-up is finished, deploy the `OpenStackDataPlaneDeployment` CR:
+
[source,bash,role=execute,subs=attributes]
----
cat << EOF | oc apply -f -
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: compute-adoption
spec:
  nodeSets:
  - compute-2
  - compute-3
EOF
----

. You should see that *compute02* jobs are progressing, however *compute03* fails in the redhat service that takes care of the subscription
+
[source,bash,role=execute,subs=attributes]
----
oc get jobs -n openstack
----

----
NAME                                   COMPLETIONS   DURATION   AGE
bootstrap-compute-adoption-compute-2   0/1           29s        29s
keystone-cron-29050201                 1/1           6s         8m38s
redhat-compute-adoption-compute-2      1/1           3m14s      3m43s
redhat-compute-adoption-compute-3      0/1           3m43s      3m43s
----
. If we check the *compute03* logs, there are errors to download the RPM packages
+
[source,bash,role=execute,subs=attributes]
----
oc logs job/redhat-compute-adoption-compute-3 -n openstack
----
----
TASK [redhat.rhel_system_roles.rhc : Handle system subscription] ***************
task path: /usr/share/ansible/collections/ansible_collections/redhat/rhel_system_roles/roles/rhc/tasks/main.yml:15
included: /usr/share/ansible/collections/ansible_collections/redhat/rhel_system_roles/roles/rhc/tasks/subscription-manager.yml for compute03

TASK [redhat.rhel_system_roles.rhc : Ensure required packages are installed] ***
task path: /usr/share/ansible/collections/ansible_collections/redhat/rhel_system_roles/roles/rhc/tasks/subscription-manager.yml:3
fatal: [compute03]: FAILED! => {"changed": false, "msg": "Failed to download metadata for repo 'openstack-17.1-for-rhel-9-x86_64-rpms': Cannot download repomd.xml: Cannot download repodata/repomd.xml: All mirrors were tried", "rc": 1, "results": []}

NO MORE HOSTS LEFT *************************************************************

NO MORE HOSTS LEFT *************************************************************

PLAY RECAP *********************************************************************
compute03                  : ok=5    changed=0    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0
----

== Option1: Recover compute03 

. If you want to recover the *compute03* connect to the *compute03*:
+
[source,bash,role=execute,subs=attributes]
----
ssh -i /home/lab-user/.ssh/{guid}key.pem cloud-user@compute03
----
. Revert back the DNS configuration:
+
[source,bash,role=execute,subs=attributes]
----
sudo cp /root/resolv.conf.bck /etc/resolv.conf
----
. Delete the facts.d folder as it's used to mark the execution of bootstrap_command as completed. By deleting the bootstrap command can be reexecuted:
+
[source,bash,role=execute,subs=attributes]
----
sudo rm -rf /etc/ansible/facts.d
----
. Create a new OpenStackDataPlaneDeployment adding only the *compute-3* nodeset corresponding to the *compute03*
+
[source,bash,role=execute,subs=attributes]
----
cat << EOF | oc apply -f -
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-edpm-compute-recover-3
spec:
  nodeSets:
  - compute-3
EOF
----
. You should see that now *compute03* jobs are progressing
+
[source,bash,role=execute,subs=attributes]
----
oc get jobs -n openstack
----

.Verification

. Confirm that all the Ansible EE pods reach a `Completed` status:
+
[source,bash,role=execute,subs=attributes]
----
watch oc get pod -l app=openstackansibleee
----
+
[source,bash,role=execute,subs=attributes]
----
oc logs -l app=openstackansibleee -f --max-log-requests 30
----

. Wait for the data plane node set to reach the `Ready` status:
+
[source,bash,role=execute,subs=attributes]
----
oc wait --for condition=Ready osdpns/openstack-edpm-compute-recover-3 --timeout=30m
----

. Verify that the {networking_first_ref} agents are running:
+
[source,bash,role=execute,subs=attributes]
----
oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                   | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| 174fc099-5cc9-4348-b8fc-59ed44fcfb0e | DHCP agent                   | standalone.localdomain | nova              | :-)   | UP    | neutron-dhcp-agent         |
| 10482583-2130-5b0d-958f-3430da21b929 | OVN Metadata agent           | standalone.localdomain |                   | :-)   | UP    | neutron-ovn-metadata-agent |
| a4f1b584-16f1-4937-b2b0-28102a3f6eaa | OVN Controller agent         | standalone.localdomain |                   | :-)   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
----

== Option2: Remove compute03 from your deployment

+
If you do not want to recover the *compute03* but to remove the *compute03* from your cloud, in the bastion, list the compute services:
+
[source,bash,role=execute,subs=attributes]
----
alias openstack="oc exec -t openstackclient -- openstack"
openstack compute service list
----
----
+--------------------------------------+----------------+------------------------+----------+---------+-------+----------------------------+
| ID                                   | Binary         | Host                   | Zone     | Status  | State | Updated At                 |
+--------------------------------------+----------------+------------------------+----------+---------+-------+----------------------------+
| a1419dda-fff2-4da6-8e01-94b20ffe5ecd | nova-conductor | nova-cell0-conductor-0 | internal | enabled | up    | 2025-03-26T20:53:04.000000 |
| ce2371aa-a0d6-4e17-b7ca-0a2eb54b4aef | nova-scheduler | nova-scheduler-0       | internal | enabled | up    | 2025-03-26T20:53:06.000000 |
| cdb3c0b1-ccd0-43de-a2b2-754b10e5627b | nova-compute   | compute02.localdomain  | nova     | enabled | up    | 2025-03-26T20:53:06.000000 |
| 092f5d2d-f4fb-48c2-8f55-fa83ceaa9f7a | nova-compute   | compute03.localdomain  | nova     | enabled | down  | 2025-03-26T16:59:12.000000 |
| 5db4c90e-3cbb-4ba6-890d-5487e8e0b7fc | nova-conductor | nova-cell1-conductor-0 | internal | enabled | up    | 2025-03-26T20:53:12.000000 |
+--------------------------------------+----------------+------------------------+----------+---------+-------+----------------------------+
----

. Delete the VM hosted in the *compute03* :
+
[source,bash,subs=attributes]
----
openstack server delete test-server-compute-03
----

. Delete the *compute03* compute service:
+
[source,bash,subs=attributes]
----
openstack compute service delete UUID_of_compute03
----

.Next steps

* You must perform a fast-forward upgrade on your Compute services. For more information, see xref:performing-a-fast-forward-upgrade-on-compute-services_{context}[Performing a fast-forward upgrade on Compute services].
